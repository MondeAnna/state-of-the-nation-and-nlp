{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65bf00e",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h1 align=\"center\">The</h1>\n",
    "<h1 align=\"center\">Natural Language Processing</h1>\n",
    "<h5 align=\"center\">of</h5>\n",
    "<h1 align=\"center\">South Africa's State of the Nation</h1>\n",
    "<h1 align=\"center\">Addresses</h1>\n",
    "<h5 align=\"center\">by</h5>\n",
    "<h3 align=\"center\">Monde Anna</h3>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b31be8",
   "metadata": {},
   "source": [
    "<p>Within the following notebook, we will get to explore thte priorities and challenges faced by our country over the last 23 years, that is, from the year 2000 up to and including the year 2022. Each president and as such each presidential term, will offer us insight into said priorities as well as provide a launchpad from which to gain an insight into what the core focuses of each presidential period term.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecb979",
   "metadata": {},
   "source": [
    "<h3 align=\"Center\">Sources</h3>\n",
    "\n",
    "<br />\n",
    "\n",
    "<ul>\n",
    "    <br />\n",
    "    <li>\n",
    "        <a href=\"http://syllabus.africacode.net/projects/data-science-specific/natural-language-processing/\">Brief</a>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li><a href=\"https://www.gov.za/state-nation-address\">SONA Speech Archive</a>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9de9c",
   "metadata": {},
   "source": [
    "<h3 align=\"Center\">Imports</h3>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ec3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from string import punctuation\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import bs4\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea833c11",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h3 align=\"Center\">Global Settings</h3>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d36f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD = \"bold\"\n",
    "N_COLS = 3\n",
    "N_ROWS = 9\n",
    "SEED = 69\n",
    "\n",
    "sns.set(rc={\n",
    "    \"axes.labelpad\": 12,\n",
    "    \"axes.labelweight\": BOLD,\n",
    "    \"axes.titlepad\": 24,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.titleweight\": BOLD,\n",
    "    \"figure.figsize\": (12,6),\n",
    "    \"figure.titlesize\": 32,\n",
    "    \"figure.titleweight\": BOLD,\n",
    "})\n",
    "\n",
    "nltk_data = [\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"omw-1.4\",\n",
    "    \"punkt\",\n",
    "    \"stopwords\",\n",
    "    \"tagsets\",\n",
    "    \"wordnet\",\n",
    "]\n",
    "\n",
    "for datum in nltk_data:\n",
    "    nltk.download(datum, quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc98f32",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h2 align=\"Center\">Data Acquisition</h2>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<p>With the use of the url addresses provided in the <i>json</i> file accompanying this project, we will scrape the State of the Nation speech archive website for each transcript of the addresses as of the year 2000. Of particular importance to us are the <b><i>Date</i></b>, <b><i>The President</i></b> making the speech and the speech's <b><i>Transcript</i></b>.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64478edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_president(soup):\n",
    "    \"\"\"we concern ourselves with 21st century presidents\"\"\"\n",
    "    presidents = {\n",
    "        \"Ramaphosa\": \"Cyril Ramaphosa\",\n",
    "        \"Zuma\": \"Jacob Zuma\",\n",
    "        \"Motlanthe\": \"Kgalema Motlanthe\",\n",
    "        \"Mbeki\": \"Thabo Mbeki\",\n",
    "    }\n",
    "\n",
    "    title = \"\".join(t for t in soup.find(class_=\"title\"))\n",
    "\n",
    "    return \"\".join(\n",
    "        presidents[pres]\n",
    "        for pres in presidents.keys()\n",
    "        if pres.lower() in title.lower()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f002727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup):\n",
    "    str_date = soup.find(class_=\"field-item even\").text\n",
    "    return datetime.strptime(str_date, \"%d %b %Y\").date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01261b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech(soup):\n",
    "    return \" \".join(\n",
    "        s.text\n",
    "        for s in soup.find(class_=\"section section-content\").find_all(\"p\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40e57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_from_url(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = bs4.BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    return {\n",
    "        \"president\": get_president(soup),\n",
    "        \"date\": get_date(soup),\n",
    "        \"speech\": get_speech(soup)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfb314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/speech_urls.json\", \"r\") as file:\n",
    "    speech_urls = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7721cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_urls = [\n",
    "    url\n",
    "    for year in speech_urls\n",
    "    for urls in year.values()\n",
    "    for url in urls\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5d52a",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Web Scrapping</h5>\n",
    "\n",
    "<br />\n",
    "\n",
    "<p>Sadly, our source website is slow and at times suffers a <i>504 Gateway Time-Out</i> error. As such, the scrapped data as modelled by the <i>get_speech_from_url</i> function above has been stored using Jupyter Notebook's magic method <b><i>%store</i></b>.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "971d5e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%store -r scrapped_speeches\n",
    "\n",
    "if \"scrapped_speeches\" not in globals():\n",
    "    scrapped_speeches = [get_speech_from_url(url) for url in flattened_urls]\n",
    "    %store scrapped_speeches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2666c",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h2 align=\"center\">Text Preprocessing</h2>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712bbafe",
   "metadata": {},
   "source": [
    "<h3 align=\"Center\">Data Cleaning</h3>\n",
    "\n",
    "<br />\n",
    "\n",
    "<p>In this section we will move towards cleaning the newly acquired data. Of particular concern is ensuring that we:\n",
    "    <ul>\n",
    "        <li>Set all text to lowercase</li>\n",
    "        <li>Prune digits</li>\n",
    "        <li>Say goodbye to stop words</li>\n",
    "        <li>Remove puctuation</li>\n",
    "        <li>Tokenise and lemmatize the words</li>\n",
    "        <li>From amongst all speeches, cull words that:\n",
    "            <ol>\n",
    "                <li>Occurs in the top ten list per speech, for every speech</li>\n",
    "                <li>Offer little insight into the <b><i>State of the Nation</i></b>, such as remove:\n",
    "                    <ul>\n",
    "                        <li>Greetings</li>\n",
    "                        <li>Obvious pleasantries</li>\n",
    "                        <li>Titles</li>\n",
    "                        <li>References to the chair of the house</li>\n",
    "                        <li><i>etc...</i></li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ol>\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43ff617",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/pos_wordnet_map.json\", \"r\") as file:\n",
    "    POS_MAP = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d940e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(speech, *, pos_map=POS_MAP):\n",
    "    return [\n",
    "        nltk.stem.WordNetLemmatizer().lemmatize(\n",
    "            word=word,\n",
    "            pos=pos_map.get(pos_tag, \"n\")\n",
    "        )\n",
    "        for word, pos_tag in nltk.pos_tag(speech)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c22ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_word_counter(speeches):\n",
    "    \"\"\" should 'speeches' be a list, then the length is found,\n",
    "        elsewise, should 'speeches' be a string, it is split at\n",
    "        its whitespaces before finding the length of the speech\n",
    "    \"\"\"\n",
    "    return sum(\n",
    "        len(set(speech))\n",
    "        if type(speech) == list\n",
    "        else len(set(re.split(\"\\s\", speech)))\n",
    "        for speech in speeches\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bafd4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconcatinate_words(speech):\n",
    "    \"\"\"adds space when words have no spaces between them\"\"\"\n",
    "    for match in re.findall(\"([a-z][A-Z])\", speech):\n",
    "        speech = re.sub(match, f\"{match[0]} {match[1]}\", speech)\n",
    "    return speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b935f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_speech = lambda speech: \" \".join(speech)\n",
    "remove_digits = lambda speech: re.sub(\"[\\w|\\s]*\\d+[\\w|\\s]*\", \"\", speech)\n",
    "remove_punctuation = lambda speech: re.sub(f\"[{punctuation}]+\", \" \", speech)\n",
    "remove_stop_words = lambda speech, stop_words: [\n",
    "    word for word in speech if word not in stop_words\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d076445",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h3 align=\"Center\">Corpus</h3>\n",
    "<h5 align=\"Center\">Initial</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb503207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>Madame Speaker and Deputy Speaker,Chairperson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-09</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>Madame Speaker and Deputy Speaker,Deputy Presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-08</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>Madame Speaker and Deputy Speaker,Chairperson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-02-14</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>Madame Speaker of the National Assembly;Chairp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-06</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>6 February 2004 Madame SpeakerChairperson of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              president                                             speech\n",
       "date                                                                      \n",
       "2000-02-04  Thabo Mbeki  Madame Speaker and Deputy Speaker,Chairperson ...\n",
       "2001-02-09  Thabo Mbeki  Madame Speaker and Deputy Speaker,Deputy Presi...\n",
       "2002-02-08  Thabo Mbeki  Madame Speaker and Deputy Speaker,Chairperson ...\n",
       "2003-02-14  Thabo Mbeki  Madame Speaker of the National Assembly;Chairp...\n",
       "2004-02-06  Thabo Mbeki  6 February 2004 Madame SpeakerChairperson of t..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches = pd.DataFrame(scrapped_speeches)\n",
    "speeches.set_index(keys=\"date\", inplace=True)\n",
    "speeches.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c674a",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Speeches</h5>\n",
    "<h5 align=\"Center\">Add Whitespace between Individual Words</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<p>As with the <a href=\"https://www.gov.za/node/537677\">first speech of 2004</a>, often times the president begins by greeting the house. The transcript display each greeting as a new line. Consequently, words get concatenated with no space or special character to mark where each word begins and the other ends. As luck would have it, the letter casing is left undamaged and here, we solve for this issue.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ea1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"SpeakerChairperson\" in speeches.speech[4]\n",
    "assert \"ProvincesDeputy\" in speeches.speech[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e39eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_prior_deconcatination = unique_word_counter(speeches.speech)\n",
    "speeches.speech = speeches.speech.apply(deconcatinate_words)\n",
    "word_count_post_deconcatination = unique_word_counter(speeches.speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40123a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"SpeakerChairperson\" not in speeches.speech[4]\n",
    "assert \"Speaker Chairperson\" in speeches.speech[4]\n",
    "\n",
    "assert \"ProvincesDeputy\" not in speeches.speech[4]\n",
    "assert \"Provinces Deputy\" in speeches.speech[4]\n",
    "\n",
    "assert word_count_prior_deconcatination < word_count_post_deconcatination, \"\"\"\\\n",
    "expected more words as a result of splitting incorrectly merged words\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3eb35",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Speeches</h5>\n",
    "<h5 align=\"Center\">Set Text to Lowercase</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820630c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches.speech = speeches.speech.str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342063d",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Speeches</h5>\n",
    "<h5 align=\"Center\">Remove Digits</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a9fc776",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches.speech = speeches.speech.apply(remove_digits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9c4fd",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Speeches</h5>\n",
    "<h5 align=\"Center\">Removal of Punctuation</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ba8295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches.speech = speeches.speech.apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c2664",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Speeches</h5>\n",
    "<h5 align=\"Center\">Tokenize Words</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "780dda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches.speech = speeches.speech.apply(nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6262ac",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Speeches</h5>\n",
    "<h5 align=\"Center\">Remove Stop Words</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "921c5e19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words_nltk = nltk.corpus.stopwords.words(\"english\")\n",
    "stop_words_additional = [\"mr\", \"mrs\", \"\", \" \"]\n",
    "stop_words = stop_words_nltk + stop_words_additional\n",
    "\n",
    "speeches.speech = speeches.speech.apply(\n",
    "    remove_stop_words, args=(stop_words,)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e6457",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Speeches</h5>\n",
    "<h5 align=\"Center\">Lemmatization</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<p>Here we will use Part-of-Speech (POS) Tags to determine which words to apply lemmatization on. Follow the provided link for the complete <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">Penn Treebank Project</a> list. In essence, the drive is to make words <b><i>singular</i></b> and <b><i>present tense</i></b>. A great limitation with our approach is that we mostly will garner great results with <i>ASCII</i> characters as well as have some success with <i>UTF-8</i> character sets; anything beyond, such as <a href=\"https://en.wikipedia.org/wiki/ISO/IEC_8859-1\">ISO/IEC 8859-1</a>, may suffer as a result of this approach.</p>\n",
    "\n",
    "<p>With great luck the prior preprocessing has left us with these variations:\n",
    "    <ul>\n",
    "        <li>Adjectives</li>\n",
    "        <li>Adverbs</li>\n",
    "        <li>Nouns</li>\n",
    "        <li>Verbs</li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a2d1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"member\" in speeches.speech[0] and \"members\" in speeches.speech[0]\n",
    "assert \"programme\" in speeches.speech[0] and \"programmes\" in speeches.speech[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0846df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_prior_lemmatization = unique_word_counter(speeches.speech)\n",
    "speeches.speech = speeches.speech.apply(lemmatize).apply(lemmatize)\n",
    "word_count_post_lemmatization = unique_word_counter(speeches.speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e485d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"members\" not in speeches.speech[0]\n",
    "assert \"member\" in speeches.speech[0]\n",
    "\n",
    "assert \"programmes\" not in speeches.speech[0]\n",
    "assert \"programme\" in speeches.speech[0]\n",
    "\n",
    "assert word_count_post_lemmatization < word_count_prior_lemmatization, \"\"\"\\\n",
    "expect standardisation to lemma to reduce word count\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76a186",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h3 align=\"Center\">Corpus</h3>\n",
    "<h5 align=\"Center\">Interim</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fcfb2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>speech</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>madame speaker deputy speaker chairperson depu...</td>\n",
       "      <td>[madame, speaker, deputy, speaker, chairperson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-09</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>madame speaker deputy speaker deputy president...</td>\n",
       "      <td>[madame, speaker, deputy, speaker, deputy, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-08</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>madame speaker deputy speaker chairperson depu...</td>\n",
       "      <td>[madame, speaker, deputy, speaker, chairperson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-02-14</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>madame speaker national assembly chairperson n...</td>\n",
       "      <td>[madame, speaker, national, assembly, chairper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-06</th>\n",
       "      <td>Thabo Mbeki</td>\n",
       "      <td>general leader public service president mandel...</td>\n",
       "      <td>[general, leader, public, service, president, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              president                                             speech  \\\n",
       "date                                                                         \n",
       "2000-02-04  Thabo Mbeki  madame speaker deputy speaker chairperson depu...   \n",
       "2001-02-09  Thabo Mbeki  madame speaker deputy speaker deputy president...   \n",
       "2002-02-08  Thabo Mbeki  madame speaker deputy speaker chairperson depu...   \n",
       "2003-02-14  Thabo Mbeki  madame speaker national assembly chairperson n...   \n",
       "2004-02-06  Thabo Mbeki  general leader public service president mandel...   \n",
       "\n",
       "                                                    tokenized  \n",
       "date                                                           \n",
       "2000-02-04  [madame, speaker, deputy, speaker, chairperson...  \n",
       "2001-02-09  [madame, speaker, deputy, speaker, deputy, pre...  \n",
       "2002-02-08  [madame, speaker, deputy, speaker, chairperson...  \n",
       "2003-02-14  [madame, speaker, national, assembly, chairper...  \n",
       "2004-02-06  [general, leader, public, service, president, ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches[\"tokenized\"] = speeches.speech.copy(deep=True)\n",
    "speeches.speech = speeches.speech.apply(\n",
    "    concat_speech\n",
    ")\n",
    "\n",
    "speeches.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c102c",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h3 align=\"Center\">Document-Term Matrix</h3>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daca7c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>aasroc</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abadala</th>\n",
       "      <th>abadlwengula</th>\n",
       "      <th>abahlala</th>\n",
       "      <th>abahloniphekileyo</th>\n",
       "      <th>abakwenzayo</th>\n",
       "      <th>abalimi</th>\n",
       "      <th>abamhlophe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-09</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-08</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-02-14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-06</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            aaron  aasroc  ab  aba  ababa  abadala  abadlwengula  abahlala  \\\n",
       "date                                                                         \n",
       "2000-02-04      0       0   1    0      0        0             0         0   \n",
       "2001-02-09      0       0   0    0      0        0             0         0   \n",
       "2002-02-08      0       0   0    0      0        0             0         0   \n",
       "2003-02-14      0       0   0    0      0        0             0         0   \n",
       "2004-02-06      0       0   0    0      0        0             0         0   \n",
       "\n",
       "            abahloniphekileyo  abakwenzayo  abalimi  abamhlophe  \n",
       "date                                                             \n",
       "2000-02-04                  0            0        0           0  \n",
       "2001-02-09                  0            0        0           0  \n",
       "2002-02-08                  0            0        0           0  \n",
       "2003-02-14                  0            0        0           0  \n",
       "2004-02-06                  0            0        0           0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "vector = vectorizer.fit_transform(speeches.speech).toarray()\n",
    "\n",
    "doc_term_matrix = pd.DataFrame(\n",
    "    data=vector,\n",
    "    index=speeches.index,\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    ")\n",
    "\n",
    "doc_term_matrix.iloc[:5, :12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b28080",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h3 align=\"center\">Data Cleaning Cont.</h3>\n",
    "<h5 align=\"center\">Redundancies</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<p>For the most part, formatting and removing none alphabetic characters looks to have been completed. To our benefit, the <i>Interim Corpus</i> offers a self evident pattern; Thabo Mbeki tends to start his speeches in a similar way, as such we should be weary of words/phrases being repeated between documents and perhaps even within a document--yet offering little relation to any core insights that may be drawn from the speech.</p>\n",
    "\n",
    "<p>Arbitrarily, (a) the <i>Top-Ten</i> reoccuring words between the speeches will be our target as well as (b) senseless words with regard to the core subject matter; The State of the Nation.</p>\n",
    "\n",
    "<p>Let us attend to these matters.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a18a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_words(dtm, word_finder, end_year_iloc=-1):\n",
    "    this_year = dtm.index[end_year_iloc]\n",
    "    \n",
    "    if this_year == dtm.index[0]:\n",
    "        return word_finder(dtm=dtm, year=this_year)\n",
    "\n",
    "    return word_finder(dtm=dtm, year=this_year).intersection(\n",
    "        get_redundant_words(\n",
    "            dtm=dtm, \n",
    "            word_finder=word_finder,\n",
    "            end_year_iloc=end_year_iloc-1\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f267b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_of_words_from_current_year(dtm, year):\n",
    "    used_words_mask = dtm.loc[year] > 0\n",
    "    used_words = dtm.loc[year][used_words_mask].index\n",
    "    return set(used_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e542e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_of_top_ten_words_for_the_year(dtm, year):\n",
    "    top_ten_words = dtm.loc[year].nlargest(10).index.to_list()\n",
    "    return set(top_ten_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf8612",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"center\">Culling Top-Ten Redundancies</h5>\n",
    "<h5 align=\"Center\">Across All Speeches</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "</p>In plain English, we will:\n",
    "    <ul>\n",
    "        <li>Take the <i>Top-Ten</i> words per speech</li>\n",
    "        <li>Use set intersection while recursing toward the first speech</li>\n",
    "        <li>The remaining words will have logically been in the <i>Top-Ten</i> of each speech in order for said words to carry over the chained intersections</li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6946d6bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert not get_redundant_words(\n",
    "    dtm=doc_term_matrix,\n",
    "    word_finder=get_set_of_top_ten_words_for_the_year,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc504a9",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Observations</h5>\n",
    "\n",
    "<br />\n",
    "\n",
    "<ul>\n",
    "    <li>Recall, there are 27 speeches between the years 2000 and 2022, inclusive</li>\n",
    "    <br />\n",
    "    <li>Each year's <i>Top-Ten</i> creates the base from which to remove words that are not repeated in the list from the previous year.</li>\n",
    "    <br />\n",
    "    <li>One of the great benefits is that this becomes an approximately <i>O(n<sup>2</sup>)</i> operation as opposed to <i>O(n<sup>n</sup>)</i>; this occurs because each year is compared to fewer words in an established set, versus a new set.</li>\n",
    "    <br />\n",
    "    <li>As per the assertion above, there are no words that appear in the <i>Top-Ten</i> of each year, every year</li>\n",
    "</ul>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb479d",
   "metadata": {},
   "source": [
    "<h5 align=\"center\">Culling Senseless Redundancies</h5>\n",
    "<h5 align=\"Center\">Across All Speeches</h5>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "</p>As far as the senseless words:\n",
    "    <ul>\n",
    "        <li>We will take all words per speech</li>\n",
    "        <li>Recursively cull words that do not appear in each speech</li>\n",
    "        <li>Make a quick manual selection of words to regard as senseless</li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80997750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>action</td>\n",
       "      <td>address</td>\n",
       "      <td>africa</td>\n",
       "      <td>african</td>\n",
       "      <td>area</td>\n",
       "      <td>build</td>\n",
       "      <td>business</td>\n",
       "      <td>challenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>community</td>\n",
       "      <td>continue</td>\n",
       "      <td>country</td>\n",
       "      <td>create</td>\n",
       "      <td>development</td>\n",
       "      <td>economic</td>\n",
       "      <td>economy</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ensure</td>\n",
       "      <td>good</td>\n",
       "      <td>government</td>\n",
       "      <td>growth</td>\n",
       "      <td>health</td>\n",
       "      <td>high</td>\n",
       "      <td>implementation</td>\n",
       "      <td>improve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>include</td>\n",
       "      <td>infrastructure</td>\n",
       "      <td>institution</td>\n",
       "      <td>issue</td>\n",
       "      <td>job</td>\n",
       "      <td>land</td>\n",
       "      <td>lead</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>like</td>\n",
       "      <td>local</td>\n",
       "      <td>make</td>\n",
       "      <td>member</td>\n",
       "      <td>national</td>\n",
       "      <td>need</td>\n",
       "      <td>new</td>\n",
       "      <td>opportunity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>partnership</td>\n",
       "      <td>people</td>\n",
       "      <td>place</td>\n",
       "      <td>policy</td>\n",
       "      <td>president</td>\n",
       "      <td>private</td>\n",
       "      <td>programme</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>province</td>\n",
       "      <td>public</td>\n",
       "      <td>resource</td>\n",
       "      <td>sector</td>\n",
       "      <td>security</td>\n",
       "      <td>service</td>\n",
       "      <td>small</td>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>south</td>\n",
       "      <td>state</td>\n",
       "      <td>strengthen</td>\n",
       "      <td>support</td>\n",
       "      <td>thank</td>\n",
       "      <td>time</td>\n",
       "      <td>union</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>work</td>\n",
       "      <td>world</td>\n",
       "      <td>year</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             \\\n",
       "        action         address       africa  african         area     build   \n",
       "     community        continue      country   create  development  economic   \n",
       "        ensure            good   government   growth       health      high   \n",
       "       include  infrastructure  institution    issue          job      land   \n",
       "          like           local         make   member     national      need   \n",
       "   partnership          people        place   policy    president   private   \n",
       "      province          public     resource   sector     security   service   \n",
       "         south           state   strengthen  support        thank      time   \n",
       "          work           world         year        -            -         -   \n",
       "\n",
       "                                \n",
       "         business    challenge  \n",
       "          economy    education  \n",
       "   implementation      improve  \n",
       "             lead         life  \n",
       "              new  opportunity  \n",
       "        programme      provide  \n",
       "            small       social  \n",
       "            union        woman  \n",
       "                -            -  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_all_speeches = get_redundant_words(\n",
    "    dtm=doc_term_matrix,\n",
    "    word_finder=get_set_of_words_from_current_year,\n",
    ")\n",
    "words_in_all_speeches = sorted(words_in_all_speeches)\n",
    "\n",
    "ncols = len(words_in_all_speeches) / N_ROWS\n",
    "ncols = np.math.ceil(ncols)\n",
    "\n",
    "while (len(words_in_all_speeches) // N_ROWS) < ncols:\n",
    "    words_in_all_speeches.append(\"-\")\n",
    "\n",
    "words_in_all_speeches = np.array(\n",
    "    words_in_all_speeches\n",
    ").reshape(N_ROWS, ncols)\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=words_in_all_speeches,\n",
    "    index=[\" \"] * N_ROWS,\n",
    "    columns=[\" \"] * ncols,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0edaf",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Observations</h5>\n",
    "\n",
    "<br />\n",
    "\n",
    "<ul>\n",
    "    <li><b><i>Address</i></b>, <b><i>Africa(n)</i></b>, <b><i>Country</i></b>, <b><i>National</i></b>, <b><i>President</i></b>, <b><i>South</i></b> and <b><i>Thank</i></b> all look to be fillers within the speeches</li>\n",
    "    <br />\n",
    "    <li>The words <b><i>Action</i></b>, <b><i>Build</i></b>, <b><i>Create</i></b>, <b><i>Make</i></b> can be said to be synonyms, as such I will be keeping <b><i>Build</i></b> as I find it easier to apply to abstract concepts such as <i>\"build a sense of union\"</i> as well as more concrete concerns such as <i>\"in building more concrete and sustainable sources of energy, we offer ourselves an easier time at building the wealth and prosperity of our country\"</i></li>\n",
    "    <br />\n",
    "    <li>The words <b><i>Like</i></b> and <b><i>Year</i></b> feel as though they fell through the cracks during the initial removal of stop words</li>\n",
    "</ul>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4feaeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillers = [\n",
    "    \"address\",\n",
    "    \"africa\",\n",
    "    \"african\",\n",
    "    \"country\",\n",
    "    \"national\",\n",
    "    \"president\",\n",
    "    \"south\",\n",
    "    \"thank\"\n",
    "]\n",
    "\n",
    "synonyms_ = [\n",
    "    \"action\",\n",
    "    \"create\",\n",
    "    \"make\"\n",
    "]\n",
    "\n",
    "misc_words = [\"like\", \"year\"]\n",
    "\n",
    "for list_ in [fillers, synonyms_, misc_words]:\n",
    "    stop_words.extend(list_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36350a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speeches.tokenized = speeches.tokenized.apply(\n",
    "    remove_stop_words, args=(stop_words,)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af8d4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_len_prior_cull = doc_term_matrix.shape[1]\n",
    "\n",
    "senseless_redundancies = [\n",
    "    word\n",
    "    for word in stop_words\n",
    "    if word in doc_term_matrix.columns\n",
    "]\n",
    "\n",
    "doc_term_matrix.drop(columns=senseless_redundancies, inplace=True)\n",
    "dtm_len_post_cull = doc_term_matrix.shape[1]\n",
    "\n",
    "assert dtm_len_prior_cull > dtm_len_post_cull\n",
    "\n",
    "speeches.speech = speeches.tokenized.apply(\n",
    "    concat_speech\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
