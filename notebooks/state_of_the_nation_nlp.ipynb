{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65bf00e",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h1 align=\"center\">The</h1>\n",
    "<h1 align=\"center\">Natural Language Processing</h1>\n",
    "<h5 align=\"center\">of</h5>\n",
    "<h1 align=\"center\">South Africa's State of the Nation</h1>\n",
    "<h1 align=\"center\">Addresses</h1>\n",
    "<h5 align=\"center\">by</h5>\n",
    "<h3 align=\"center\">Monde Anna</h3>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b31be8",
   "metadata": {},
   "source": [
    "<p>Within the following notebook, we will get to explore thte priorities and challenges faced by our country over the last 23 years, that is, from the year 2000 up to and including the year 2022. Each president and as such each presidential term, will offer us insight into said priorities as well as provide a launchpad from which to gain an insight into what the core focuses of each presidential period term.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecb979",
   "metadata": {},
   "source": [
    "<h3 align=\"Center\">Sources</h3>\n",
    "\n",
    "<br />\n",
    "\n",
    "<ul>\n",
    "    <br />\n",
    "    <li>\n",
    "        <a href=\"http://syllabus.africacode.net/projects/data-science-specific/natural-language-processing/\">Brief</a>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li><a href=\"https://www.gov.za/state-nation-address\">SONA Speech Archive</a>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9de9c",
   "metadata": {},
   "source": [
    "<h3 align=\"Center\">Imports</h3>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ec3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from string import punctuation\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import bs4\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea833c11",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h3 align=\"Center\">Global Settings</h3>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d36f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD = \"bold\"\n",
    "N_COLS = 3\n",
    "N_ROWS = 9\n",
    "SEED = 69\n",
    "\n",
    "sns.set(rc={\n",
    "    \"axes.labelpad\": 12,\n",
    "    \"axes.labelweight\": BOLD,\n",
    "    \"axes.titlepad\": 24,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.titleweight\": BOLD,\n",
    "    \"figure.figsize\": (12,6),\n",
    "    \"figure.titlesize\": 32,\n",
    "    \"figure.titleweight\": BOLD,\n",
    "})\n",
    "\n",
    "nltk_data = [\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"omw-1.4\",\n",
    "    \"punkt\",\n",
    "    \"stopwords\",\n",
    "    \"tagsets\",\n",
    "    \"wordnet\",\n",
    "]\n",
    "\n",
    "for datum in nltk_data:\n",
    "    nltk.download(datum, quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc98f32",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h2 align=\"Center\">Data Acquisition</h2>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<p>With the use of the url addresses provided in the <i>json</i> file accompanying this project, we will scrape the State of the Nation speech archive website for each transcript of the addresses as of the year 2000. Of particular importance to us are the <b><i>Date</i></b>, <b><i>The President</i></b> making the speech and the speech's <b><i>Transcript</i></b>.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64478edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_president(soup):\n",
    "    \"\"\"we concern ourselves with 21st century presidents\"\"\"\n",
    "    presidents = {\n",
    "        \"Ramaphosa\": \"Cyril Ramaphosa\",\n",
    "        \"Zuma\": \"Jacob Zuma\",\n",
    "        \"Motlanthe\": \"Kgalema Motlanthe\",\n",
    "        \"Mbeki\": \"Thabo Mbeki\",\n",
    "    }\n",
    "\n",
    "    title = \"\".join(t for t in soup.find(class_=\"title\"))\n",
    "\n",
    "    return \"\".join(\n",
    "        presidents[pres]\n",
    "        for pres in presidents.keys()\n",
    "        if pres.lower() in title.lower()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f002727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup):\n",
    "    str_date = soup.find(class_=\"field-item even\").text\n",
    "    return datetime.strptime(str_date, \"%d %b %Y\").date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01261b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech(soup):\n",
    "    return \" \".join(\n",
    "        s.text\n",
    "        for s in soup.find(class_=\"section section-content\").find_all(\"p\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40e57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_from_url(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = bs4.BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    return {\n",
    "        \"president\": get_president(soup),\n",
    "        \"date\": get_date(soup),\n",
    "        \"speech\": get_speech(soup)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfb314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/speech_urls.json\", \"r\") as file:\n",
    "    speech_urls = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7721cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_urls = [\n",
    "    url\n",
    "    for year in speech_urls\n",
    "    for urls in year.values()\n",
    "    for url in urls\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5d52a",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<h5 align=\"Center\">Web Scrapping</h5>\n",
    "\n",
    "<br />\n",
    "\n",
    "<p>Sadly, our source website is slow and at times suffers a <i>504 Gateway Time-Out</i> error. As such, the scrapped data as modelled by the <i>get_speech_from_url</i> function above has been stored using Jupyter Notebook's magic method <b><i>%store</i></b>.</p>\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "971d5e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%store -r scrapped_speeches\n",
    "\n",
    "if \"scrapped_speeches\" not in globals():\n",
    "    scrapped_speeches = [get_speech_from_url(url) for url in flattened_urls]\n",
    "    %store scrapped_speeches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
